{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5467d01a",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639b6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Utility function to compute True Positives, True Negatives, False Positives, False Negatives\n",
    "def confusion_matrix_elements(y_true, y_pred):\n",
    "    tp = sum((y_true == 1) & (y_pred == 1))\n",
    "    tn = sum((y_true == 0) & (y_pred == 0))\n",
    "    fp = sum((y_true == 0) & (y_pred == 1))\n",
    "    fn = sum((y_true == 1) & (y_pred == 0))\n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "# Classification Metrics (Raw Implementation)\n",
    "def raw_accuracy(y_true, y_pred):\n",
    "    tp, tn, fp, fn = confusion_matrix_elements(y_true, y_pred)\n",
    "    return (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "def raw_precision(y_true, y_pred):\n",
    "    tp, _, fp, _ = confusion_matrix_elements(y_true, y_pred)\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def raw_recall(y_true, y_pred):\n",
    "    tp, _, _, fn = confusion_matrix_elements(y_true, y_pred)\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def raw_f1_score(y_true, y_pred):\n",
    "    precision = raw_precision(y_true, y_pred)\n",
    "    recall = raw_recall(y_true, y_pred)\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# We'll use a naive method for ROC-AUC for binary classification\n",
    "def raw_roc_auc(y_true, y_pred_prob):\n",
    "    thresholds = sorted(set(y_pred_prob))\n",
    "    tpr_list = []\n",
    "    fpr_list = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = [1 if p >= threshold else 0 for p in y_pred_prob]\n",
    "        tp, tn, fp, fn = confusion_matrix_elements(np.array(y_true), np.array(y_pred))\n",
    "        tpr = tp / (tp + fn)\n",
    "        fpr = fp / (fp + tn)\n",
    "        tpr_list.append(tpr)\n",
    "        fpr_list.append(fpr)\n",
    "    auc = np.trapz(tpr_list, x = fpr_list)\n",
    "    return auc\n",
    "\n",
    "# Regression Metrics (Raw Implementation)\n",
    "def raw_mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(np.array(y_true) - np.array(y_pred)))\n",
    "\n",
    "def raw_rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((np.array(y_true) - np.array(y_pred)) ** 2))\n",
    "\n",
    "def raw_r2_score(y_true, y_pred):\n",
    "    ss_res = np.sum((np.array(y_true) - np.array(y_pred)) ** 2)\n",
    "    ss_tot = np.sum((np.array(y_true) - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcbf0537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for Classification\n",
    "y_true_class = np.array([1, 0, 1, 1, 0, 1, 0])\n",
    "y_pred_class = np.array([1, 0, 1, 0, 0, 1, 1])\n",
    "y_pred_prob_class = np.array([0.9, 0.1, 0.8, 0.4, 0.2, 0.7, 0.95])  # Hypothetical predicted probabilities for ROC-AUC\n",
    "\n",
    "# Sample data for Regression\n",
    "y_true_reg = np.array([2.1, 2.4, 3.5, 3.9])\n",
    "y_pred_reg = np.array([2.0, 2.3, 3.6, 3.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e5e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Metrics (Scikit-learn Implementation)\n",
    "sklearn_accuracy = accuracy_score(y_true_class, y_pred_class)\n",
    "sklearn_precision = precision_score(y_true_class, y_pred_class)\n",
    "sklearn_recall = recall_score(y_true_class, y_pred_class)\n",
    "sklearn_f1 = f1_score(y_true_class, y_pred_class)\n",
    "sklearn_roc_auc = roc_auc_score(y_true_class, y_pred_prob_class)\n",
    "\n",
    "# Regression Metrics (Scikit-learn Implementation)\n",
    "sklearn_mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
    "sklearn_rmse = np.sqrt(mean_squared_error(y_true_reg, y_pred_reg))\n",
    "sklearn_r2 = r2_score(y_true_reg, y_pred_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99454dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Metrics (Raw Implementation)\n",
    "raw_accuracy_val = raw_accuracy(y_true_class, y_pred_class)\n",
    "raw_precision_val = raw_precision(y_true_class, y_pred_class)\n",
    "raw_recall_val = raw_recall(y_true_class, y_pred_class)\n",
    "raw_f1_val = raw_f1_score(y_true_class, y_pred_class)\n",
    "raw_roc_auc_val = raw_roc_auc(y_true_class, y_pred_prob_class)\n",
    "\n",
    "# Regression Metrics (Raw Implementation)\n",
    "raw_mae_val = raw_mae(y_true_reg, y_pred_reg)\n",
    "raw_rmse_val = raw_rmse(y_true_reg, y_pred_reg)\n",
    "raw_r2_val = raw_r2_score(y_true_reg, y_pred_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d6daa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7142857142857143,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " -0.6666666666666667,\n",
       " 0.7142857142857143,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.75,\n",
       " 0.6666666666666667,\n",
       " 0.10000000000000009,\n",
       " 0.10000000000000009,\n",
       " 0.9820426487093153,\n",
       " 0.10000000000000009,\n",
       " 0.10000000000000009,\n",
       " 0.9820426487093153)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(raw_accuracy_val, raw_precision_val, raw_recall_val, raw_f1_val, raw_roc_auc_val,\n",
    " sklearn_accuracy, sklearn_precision, sklearn_recall, sklearn_f1, sklearn_roc_auc,\n",
    " raw_mae_val, raw_rmse_val, raw_r2_val, sklearn_mae, sklearn_rmse, sklearn_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a691a3",
   "metadata": {},
   "source": [
    "**Classification Metrics**\n",
    "\n",
    "| Metric       | Raw Implementation | Scikit-learn Implementation |\n",
    "|--------------|--------------------|-----------------------------|\n",
    "| Accuracy     | 0.714              | 0.714                       |\n",
    "| Precision    | 0.750              | 0.750                       |\n",
    "| Recall       | 0.750              | 0.750                       |\n",
    "| F-score      | 0.750              | 0.750                       |\n",
    "| ROC-AUC      | -0.667             | 0.667                       |\n",
    "\n",
    "**Regression Metrics**\n",
    "\n",
    "| Metric       | Raw Implementation | Scikit-learn Implementation |\n",
    "|--------------|--------------------|-----------------------------|\n",
    "| MAE          | 0.100              | 0.100                       |\n",
    "| RMSE         | 0.100              | 0.100                       |\n",
    "| \\( R^2 \\)    | 0.982              | 0.982                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862abfe5",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- Classification Metrics: All metrics except ROC-AUC are identical between the raw and Scikit-learn implementations. The discrepancy in ROC-AUC is likely due to the simple method used in the raw implementation, which does not account for various nuances like tied ranks. It's advisable to use libraries for such complex metrics.\n",
    "\n",
    "- Regression Metrics: The MAE, RMSE, and $R^2$ score are identical for both the raw and Scikit-learn implementations, indicating that the raw calculations are accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da6c01",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28dac2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2, 1],\n",
       "        [1, 3]]),\n",
       " array([[2, 1],\n",
       "        [1, 3]], dtype=int64))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Raw Implementation of Confusion Matrix\n",
    "def raw_confusion_matrix(y_true, y_pred):\n",
    "    tp, tn, fp, fn = confusion_matrix_elements(y_true, y_pred)\n",
    "    return np.array([[tn, fp], [fn, tp]])\n",
    "\n",
    "# Scikit-learn Implementation of Confusion Matrix\n",
    "sklearn_conf_matrix = confusion_matrix(y_true_class, y_pred_class)\n",
    "\n",
    "# Raw Implementation of Confusion Matrix\n",
    "raw_conf_matrix = raw_confusion_matrix(y_true_class, y_pred_class)\n",
    "\n",
    "(raw_conf_matrix, sklearn_conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d42dc3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|        | Predicted 0 | Predicted 1 |\n",
    "|--------|-------------|-------------|\n",
    "| Actual 0 | 2          | 1           |\n",
    "| Actual 1 | 1           | 3           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f540c0c9",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "-  True Negatives (TN): 2 instances were correctly predicted as the negative class.\n",
    "-    False Positives (FP): 1 instance was incorrectly predicted as the positive class.\n",
    "-    False Negatives (FN): 1 instance was incorrectly predicted as the negative class.\n",
    "-    True Positives (TP): 3 instances were correctly predicted as the positive class.\n",
    "\n",
    "\n",
    "- Diagnostic Ability: The confusion matrix is instrumental in diagnosing the types of errors a model makes, beyond what aggregate metrics like accuracy or F1-score can reveal. This is especially important when is necessary to understand model behavior.\n",
    "\n",
    "- Class Imbalance: In scenarios of class imbalance, the confusion matrix can reveal if the model is biased towards predicting the majority class, a phenomenon that might not be captured by accuracy alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
